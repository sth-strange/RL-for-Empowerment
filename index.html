<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <link rel="stylesheet" href="stylesheets/style.css">
    <script src="javascripts/jquery-3.5.1.js" charset="utf-8"></script>
    <script src="javascripts/script.js" charset="utf-8"></script>
    <title>RL for Empowerment: Maximized Opportunities as a Proxy for Altruism</title>
    <meta name="viewport" content="width=device-width">
    <link rel="icon" type="image/x-icon" href="images/logo.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  </head>
  <body>
    <div class="cover">
      <img src="images/cover.png" alt="cover image" class="cover">
    </div>
    <div class="container">
      <div class="heading mobile_heading">RL for&nbsp;Empowerment: Maximized <br> Opportunities as&nbsp;a&nbsp;Proxy for&nbsp;Altruism</div>
      <div class="text grey_text sign">Alexandra Rybakova, Oleg Larikov, Slava Meriton, Vladislav Konoplitskij, Anna Samina, Yohanna Krasiwski</div>
      <div class="text grey_text mobile_sign">Alexandra Rybakova, Oleg Larikov, Slava Meriton, <br>Vladislav Konoplitskij, Anna Samina, Yohanna Krasiwski</div>
      <div class="flex">
        <div class="text_size">
          <div class="text first_text">How can we transform the&nbsp;intuitive notion of&nbsp;altruism into&nbsp;a&nbsp;formal framework for&nbsp;designing reinforcement learning (RL) agents? The&nbsp;immediate challenge is&nbsp;defining a&nbsp;reward function that&nbsp;promotes altruistic behavior.</div>
          <div class="text  paragraph">At&nbsp;the&nbsp;core, however, lies a&nbsp;deeper question: Can this&nbsp;formalized version of&nbsp;altruism serve as&nbsp;a&nbsp;reliable proxy? How far can this idea be pushed? We want to&nbsp;explore where it works, where it breaks down, and in&nbsp;which contexts altruistic proxy can or&nbsp;cannot be trusted to&nbsp;produce aligned behavior.</div>
          <div class="">
            <video class = 'mobile_video' width="100%" height="auto" controls>
              <source src="images/recording.mp4" type="video/mp4">
                <source src="images/recording.webm" type="video/webm">
                  Your browser does not support the video tag.
                </video>
          </div>
          <div class="flex button_flex ">
            <a href="https://docs.google.com/presentation/d/1hk7zFTGB_XUSdW7lUj2eHLIgumzkbm5AvrFNsOKDpUI/edit?usp=sharing" class="a_button">
            <div class="button active_button width_button">
              <div class="button_text">Slides</div>
            </div>
            </a>
            <div class="button button_distance non_active width_button">
              <div class="button_text non_active_text">Paper</div>
            </div>
            <div class="button button_distance non_active width_button">
              <div class="button_text non_active_text">Code</div>
            </div>
          </div>
        </div>
        <div class="distance">
          <div class="">
            <video class = 'video' width="100%" height="auto" controls>
              <source src="images/recording.mp4" type="video/mp4">
                <source src="images/recording.webm" type="video/webm">
                  Your browser does not support the video tag.
                </video>
          </div>
        </div>
      </div>
      <div class="subtitle">Abstract</div>
      <div class="text long_text">Our work builds on&nbsp;the&nbsp;Empowerment concept from&nbsp;Klyubin et al.'s work “All Else Being Equal Be Empowered” which&nbsp;Du et al. later expanded in&nbsp;AvE: Assistance via Empowerment. We also built on&nbsp;Franzmeyer et al.’s  "Learning Altruistic Behaviours in&nbsp;Reinforcement Learning without External Rewards" with&nbsp;the&nbsp;introduction of&nbsp;the&nbsp;Choice metric. These approaches are grounded in&nbsp;the&nbsp;notion that, rather than achieving specific goals on&nbsp;our&nbsp;behalf, helpful AI should maximize our&nbsp;capacity to&nbsp;pursue a&nbsp;wide range of&nbsp;possibilities. The&nbsp;Choice metric reduces the&nbsp;need for&nbsp;an&nbsp;altruistic agent to&nbsp;possess perfect knowledge of&nbsp;its environment. It does this by&nbsp;defining the&nbsp;agent’s reward in&nbsp;terms of&nbsp;the&nbsp;number of&nbsp;possible states that its actions open up for&nbsp;the&nbsp;user. </div>
      <div class="text long_text paragraph">We are continuing this line of&nbsp;research, focusing on&nbsp;computation of&nbsp;the&nbsp;reward for&nbsp;opportunity maximizing agents. Specifically, we train so-called support agents using data from earlier steps of&nbsp;both support and&nbsp;target agent in&nbsp;the&nbsp;same episode. For&nbsp;example, if&nbsp;our planning horizon spans 10 steps, we defer calculating the&nbsp;reward for the first step until step 11, when we update the&nbsp;support agent's policy using snapshots of&nbsp;its and&nbsp;target agent's moves.</div>
      <div class="text long_text paragraph">In&nbsp;exploring different approaches to&nbsp;formalizing altruism, we believe it is crucial to&nbsp;recognize that even on&nbsp;a&nbsp;conceptual level altruism is not&nbsp;an&nbsp;end goal. It is a&nbsp;proxy. And like any&nbsp;proxy, it is vulnerable to&nbsp;Goodhart's law. The critical question driving our research is: How reliable is this proxy? What are the&nbsp;predictable failure modes we can replicate even with toy examples? As&nbsp;these are conceptual questions, we limit our investigation to&nbsp;a&nbsp;two-dimensional, discrete environment to&nbsp;facilitate clearer analysis of&nbsp;the&nbsp;underlying concepts.</div>
  <div class="subtitle">Work stages:</div>
  <div class="text long_text short_text">1. Formalization of&nbsp;the&nbsp;concept.</div>
  <div class="text grey_text short_text">Creating yet another way to&nbsp;write a&nbsp;reward function that can be used to&nbsp;train RL agents inclined toward altruistic behavior, matching human expectations of&nbsp;altruism.</div>
  <div class="text long_text paragraph short_text">2. Performance Evaluation.</div>
  <div class="text grey_text short_text">Comparing the&nbsp;performance of&nbsp;altruistic agents with prior work to&nbsp;confirm that our approach performs on&nbsp;par&nbsp;with existing methods.</div>
  <div class="text long_text paragraph short_text">3. Failure Mode Analysis.</div>
  <div class="text grey_text short_text">Investigating the&nbsp;failure mode we’ve identified as&nbsp;the&nbsp;"trade-off scenario," where altruistic behavior conflicts with the&nbsp;objectives of&nbsp;the&nbsp;patron agent.</div>
  <div class="dotted"></div>
  <div class="text long_text paragraph short_text">4. Open Framework (coming soon).</div>
  <div class="text grey_text short_text">Developing an&nbsp;open framework that allows others to&nbsp;test their own hypotheses within this&nbsp;setup.</div>
  <div class="subtitle">Join the&nbsp;project!</div>
  <div class="text long_text paragraph">Originally, this project began as&nbsp;part of&nbsp;our final project in&nbsp;courses
    <a href="https://aisafetyfundamentals.com" class="custom_link">BlueDot Impact's AI Safety Fundamentals</a>
    and
    <a href="https://www.aisafetybook.com/virtual-course" class="custom_link">AI Safety, Ethics and Society Course by Center for AI Safety</a>, but it has since evolved beyond those timelines and the capacity of our small team. We believe that exploring altruism as a metric for useful agents requires much more conceptual work than four people can accomplish in a lifetime.</div>
  <div class="text long_text paragraph">That’s why we’ve opened this project to the community. If you have ideas, feedback, or would like to stay updated on our progress, please reach out or subscribe for updates.</div>
  <div class="short_container">
    <div class="authors">
      <img src="images/authors.png" alt="authors image" class="authors">
    </div>
  </div>
  <div class="mobile_authors">
    <img src="images/authors.png" alt="mobile_authors image" class="mobile_authors">
  </div>
    </div>
    <div class="end"></div>
  </body>
</html>
